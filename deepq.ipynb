{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\python_dev\\.venv_rl_gym\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "    '''\n",
    "    Neural Net n linear layers\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_shape, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f'Forward!')\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, pretrained):\n",
    "        \n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # DQN network  \n",
    "        self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "\n",
    "        if self.pretrained:\n",
    "            self.dqn.load_state_dict(torch.load(\"DQN.pt\", map_location=torch.device(self.device)))\n",
    "        self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        #self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "        self.STATE_MEM = torch.zeros(max_memory_size, self.state_space)\n",
    "        #self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "        self.ACTION_MEM = torch.zeros(max_memory_size, self.action_space)\n",
    "        self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "        #self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "        self.STATE2_MEM = torch.zeros(max_memory_size, self.state_space)\n",
    "        self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "        self.ending_position = 0\n",
    "        self.num_in_queue = 0\n",
    "\n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        \"\"\"Store the experiences in a buffer to use later\"\"\"\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "\n",
    "\n",
    "    def batch_experiences(self):\n",
    "        \"\"\"Randomly sample 'batch size' experiences\"\"\"\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]      \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "\n",
    "\n",
    "    def random_action(self, max_fingers, max_keys):\n",
    "        action = np.zeros(max_keys, dtype=np.float32)\n",
    "\n",
    "        fingers = random.randrange(max_fingers)\n",
    "        for finger in range(fingers):\n",
    "            action[random.randrange(max_keys)] += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        \"\"\"Epsilon-greedy action\"\"\"\n",
    "        \n",
    "        num_fingers = 10\n",
    "        num_keys = self.state_space\n",
    "\n",
    "        if random.random() < self.exploration_rate:\n",
    "            #return torch.tensor([[random.randrange(self.action_space)]])\n",
    "            print(f'Random - ', end='')\n",
    "            return torch.tensor(self.random_action(num_fingers, num_keys))\n",
    "        else:\n",
    "            print(f'Exploit - ', end='')\n",
    "            q_vals = self.dqn(state.to(self.device)).cpu()\n",
    "            print(f'q_v: {q_vals.dtype} ({q_vals.size()})')\n",
    "\n",
    "            #high_indexes = torch.topk(q_vals, num_fingers).indices\n",
    "            #for c in range(len(q_vals)):\n",
    "            #    if c not in high_indexes:\n",
    "            #        q_vals[c] = 0\n",
    "\n",
    "            #return torch.argmax(q_vals).unsqueeze(0).unsqueeze(0).cpu()\n",
    "            return q_vals\n",
    "\n",
    "\n",
    "    def experience_replay(self):\n",
    "        '''\n",
    "        Perhaps try to train for every step instead...\n",
    "        '''\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "    \n",
    "        # Sample a batch of experiences\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.batch_experiences()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a) \n",
    "        target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "        current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
    "\n",
    "\n",
    "    def train(self, state, action, reward, state_next, step):\n",
    "        '''\n",
    "        Our own train algorithm\n",
    "        '''\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a)\n",
    "        \n",
    "        print(f'r: {reward.dtype} ({reward.size()}) s_n: {state_next.dtype} ({state_next.size()}) a: {action.dtype} ({action.size()}), s: {state.dtype} ({state.size()}), ', end='')\n",
    "        target = (reward + self.gamma * self.dqn(state_next.to(self.device)).cpu()).squeeze(0) #.max(0).values.unsqueeze(0)\n",
    "        current = self.dqn(state.to(self.device)).cpu()\n",
    "        \n",
    "        print(f'train step {step}, t: {target.size()} c: {current.size()}')\n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward(retain_graph=True) # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class piano_env:\n",
    "\n",
    "    def __init__(self, num_keys, num_fingers):\n",
    "        self.num_keys = num_keys\n",
    "        self.num_fingers = num_fingers\n",
    "        self.observation_space = num_keys\n",
    "        self.action_space = num_keys\n",
    "\n",
    "\n",
    "    def ext_reward(self, state):\n",
    "        '''\n",
    "        state is a np.array of velocities(?)\n",
    "        What should we reward?\n",
    "        '''\n",
    "        num_keys = len(state)\n",
    "        reward = 0\n",
    "\n",
    "        for key in state:\n",
    "            if key:\n",
    "                reward += key\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Replaces env.step()\n",
    "        Returns state_next, reward, terminal (array, float, bool)\n",
    "        '''\n",
    "        state = action #because this is very simple\n",
    "\n",
    "        return (state, self.ext_reward(state), False)\n",
    "    \n",
    "    def reset(self):\n",
    "        return np.zeros(self.num_keys, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 5.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 1, t: torch.Size([128]) c: torch.Size([128])\n",
      "1: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 2.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 2, t: torch.Size([128]) c: torch.Size([128])\n",
      "2: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 8.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 3, t: torch.Size([128]) c: torch.Size([128])\n",
      "3: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 6.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 4, t: torch.Size([128]) c: torch.Size([128])\n",
      "4: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 5.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 5, t: torch.Size([128]) c: torch.Size([128])\n",
      "5: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 6.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 6, t: torch.Size([128]) c: torch.Size([128])\n",
      "6: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 5.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 7, t: torch.Size([128]) c: torch.Size([128])\n",
      "7: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 3.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 8, t: torch.Size([128]) c: torch.Size([128])\n",
      "8: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 4.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 9, t: torch.Size([128]) c: torch.Size([128])\n",
      "9: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 1.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 10, t: torch.Size([128]) c: torch.Size([128])\n",
      "10: state torch.float32 (torch.Size([128]))Random - action: torch.float32 reward: 1.0\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 11, t: torch.Size([128]) c: torch.Size([128])\n",
      "11: state torch.float32 (torch.Size([128]))Exploit - Forward!\n",
      "q_v: torch.float32 (torch.Size([128]))\n",
      "action: torch.float32 reward: 0.3511638641357422\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 12, t: torch.Size([128]) c: torch.Size([128])\n",
      "12: state torch.float32 (torch.Size([128]))Exploit - Forward!\n",
      "q_v: torch.float32 (torch.Size([128]))\n",
      "action: torch.float32 reward: 0.37362441420555115\n",
      "r: torch.float32 (torch.Size([1, 1])) s_n: torch.float32 (torch.Size([128])) a: torch.float32 (torch.Size([128])), s: torch.float32 (torch.Size([128])), Forward!\n",
      "Forward!\n",
      "train step 13, t: torch.Size([128]) c: torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 128]], which is output 0 of AsStridedBackward0, is at version 13; expected version 12 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\python_dev\\rl_gym\\deepq.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pretrained \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#with torch.autograd.set_detect_anomaly(True):\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m run(training_mode, pretrained)\n",
      "\u001b[1;32mc:\\code\\python_dev\\rl_gym\\deepq.ipynb Cell 5\u001b[0m in \u001b[0;36mrun\u001b[1;34m(training_mode, pretrained, num_episodes, exploration_max)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m terminal \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mint\u001b[39m(terminal)])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m#agent.remember(state, action, reward, state_next, terminal)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m#agent.experience_replay()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain(state, action, reward, state_next, steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m state \u001b[39m=\u001b[39m state_next\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mif\u001b[39;00m terminal:\n",
      "\u001b[1;32mc:\\code\\python_dev\\rl_gym\\deepq.ipynb Cell 5\u001b[0m in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, state, action, reward, state_next, step)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m, t: \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m c: \u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml1(current, target)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep() \u001b[39m# Backpropagate error\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/python_dev/rl_gym/deepq.ipynb#W4sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_rate \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_decay\n",
      "File \u001b[1;32mc:\\code\\python_dev\\.venv_rl_gym\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\code\\python_dev\\.venv_rl_gym\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 128]], which is output 0 of AsStridedBackward0, is at version 13; expected version 12 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "training_mode = True\n",
    "pretrained = False\n",
    "#with torch.autograd.set_detect_anomaly(True):\n",
    "run(training_mode, pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(training_mode, pretrained, num_episodes=10, exploration_max=1):\n",
    "   \n",
    "    #env = gym.make('Breakout-v0') # can change the environmeent accordingly\n",
    "    keys = 128\n",
    "    fingers = 10\n",
    "    env = piano_env(keys, fingers)\n",
    "    #env = create_env(env)  # Wraps the environment so that frames are grayscale\n",
    "    observation_space = env.observation_space\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.2,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99,\n",
    "                     pretrained=pretrained)\n",
    "    \n",
    "    # Restart the enviroment for each episode\n",
    "    #num_episodes = num_episodes\n",
    "    #env.reset()\n",
    "\n",
    "    total_rewards = []\n",
    "    if training_mode and pretrained:\n",
    "        with open(\"total_rewards.pkl\", 'rb') as f:\n",
    "            total_rewards = pickle.load(f)\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor(state)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            print(f'{steps}: state {state.dtype} ({state.size()})', end='')\n",
    "            action = agent.act(state)\n",
    "            print(f'action: {action.dtype} ', end='')\n",
    "            steps += 1\n",
    "            \n",
    "            #state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            state_next, reward, terminal = env.step(action)\n",
    "            print(f'reward: {reward}')\n",
    "            total_reward += reward\n",
    "            #state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                #agent.remember(state, action, reward, state_next, terminal)\n",
    "                #agent.experience_replay()\n",
    "                agent.train(state, action, reward, state_next, steps)\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "            if steps >= 100:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "        if ep_num != 0 and ep_num % 100 == 0:\n",
    "            print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "        num_episodes += 1\n",
    "    \n",
    "    print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "    \n",
    "    # Save the trained memory so that we can continue from where we stop using 'pretrained' = True\n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "\n",
    "\n",
    "        torch.save(agent.dqn.state_dict(), \"DQN.pt\")  \n",
    "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
    "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
    "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
    "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
    "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
    "    \n",
    "    #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('.venv_rl_gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e6823198a8a79eced2bd754d1398553436a7da3c0669f670866c72e31e862ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
